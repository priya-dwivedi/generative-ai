{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pdwivedi/miniconda3/envs/langchain/lib/python3.11/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.8.17) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "\n",
    "## Text Splitting & Docloader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.document_loaders import TextLoader\n",
    "import chromadb\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the BGE small embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "\n",
    "bge_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load OpenAI embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "model_name = \"text-embedding-3-small\"\n",
    "\n",
    "openai_embeddings = OpenAIEmbeddings(model=model_name, allowed_special={'<|endoftext|>', '<|endofprompt|>'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "This dataset has 108 arxiv papers with content parsed using Meta's Nougat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "      <th>comment</th>\n",
       "      <th>journal_ref</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>published</th>\n",
       "      <th>updated</th>\n",
       "      <th>references</th>\n",
       "      <th>content</th>\n",
       "      <th>noref_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2206.02336</td>\n",
       "      <td>2206.02336</td>\n",
       "      <td>Making Large Language Models Better Reasoners ...</td>\n",
       "      <td>Few-shot learning is a challenging task that r...</td>\n",
       "      <td>http://arxiv.org/pdf/2206.02336</td>\n",
       "      <td>['Yifei Li' 'Zeqi Lin' 'Shizhuo Zhang' 'Qiang ...</td>\n",
       "      <td>['cs.CL' 'cs.AI']</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20220606</td>\n",
       "      <td>20230524</td>\n",
       "      <td>\\n\\n* D. Andor, L. He, K. Lee, and E. Pitler (...</td>\n",
       "      <td># Making Large Language Models Better Reasoner...</td>\n",
       "      <td># Making Large Language Models Better Reasoner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2206.04615</td>\n",
       "      <td>2206.04615</td>\n",
       "      <td>Beyond the Imitation Game: Quantifying and ext...</td>\n",
       "      <td>Language models demonstrate both quantitative ...</td>\n",
       "      <td>http://arxiv.org/pdf/2206.04615</td>\n",
       "      <td>['Aarohi Srivastava' 'Abhinav Rastogi' 'Abhish...</td>\n",
       "      <td>['cs.CL' 'cs.AI' 'cs.CY' 'cs.LG' 'stat.ML']</td>\n",
       "      <td>27 pages, 17 figures + references and appendic...</td>\n",
       "      <td>Transactions on Machine Learning Research, May...</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20220609</td>\n",
       "      <td>20230612</td>\n",
       "      <td>\\n\\n* Wikiquote et al. (2021) Wikiquote, russi...</td>\n",
       "      <td># Beyond the Imitation Game: Quantifying and e...</td>\n",
       "      <td># Beyond the Imitation Game: Quantifying and e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2206.05229</td>\n",
       "      <td>2206.05229</td>\n",
       "      <td>Measuring the Carbon Intensity of AI in Cloud ...</td>\n",
       "      <td>By providing unprecedented access to computati...</td>\n",
       "      <td>http://arxiv.org/pdf/2206.05229</td>\n",
       "      <td>['Jesse Dodge' 'Taylor Prewitt' 'Remi Tachet D...</td>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>In ACM Conference on Fairness, Accountability,...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>20220610</td>\n",
       "      <td>20220610</td>\n",
       "      <td>\\n\\n* (1)\\n* Anthony et al. (2020) Lasse F. Wo...</td>\n",
       "      <td>[MISSING_PAGE_EMPTY:1]\\n\\nIntroduction\\n\\nClim...</td>\n",
       "      <td>[MISSING_PAGE_EMPTY:1]\\n\\nIntroduction\\n\\nClim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2206.05802</td>\n",
       "      <td>2206.05802</td>\n",
       "      <td>Self-critiquing models for assisting human eva...</td>\n",
       "      <td>We fine-tune large language models to write na...</td>\n",
       "      <td>http://arxiv.org/pdf/2206.05802</td>\n",
       "      <td>['William Saunders' 'Catherine Yeh' 'Jeff Wu' ...</td>\n",
       "      <td>['cs.CL' 'cs.LG']</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20220612</td>\n",
       "      <td>20220614</td>\n",
       "      <td>(RLHP) has become more common [1, 2, 3, 4], d...</td>\n",
       "      <td># Self-critiquing models for assisting human e...</td>\n",
       "      <td># Self-critiquing models for assisting human e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2206.06336</td>\n",
       "      <td>2206.06336</td>\n",
       "      <td>Language Models are General-Purpose Interfaces</td>\n",
       "      <td>Foundation models have received much attention...</td>\n",
       "      <td>http://arxiv.org/pdf/2206.06336</td>\n",
       "      <td>['Yaru Hao' 'Haoyu Song' 'Li Dong' 'Shaohan Hu...</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>32 pages. The first three authors contribute e...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20220613</td>\n",
       "      <td>20220613</td>\n",
       "      <td>\\n\\n* Agrawal et al. (2019) Harsh Agrawal, Kar...</td>\n",
       "      <td># Language Models are General-Purpose Interfac...</td>\n",
       "      <td># Language Models are General-Purpose Interfac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          doi          id                                              title  \\\n",
       "0  2206.02336  2206.02336  Making Large Language Models Better Reasoners ...   \n",
       "1  2206.04615  2206.04615  Beyond the Imitation Game: Quantifying and ext...   \n",
       "2  2206.05229  2206.05229  Measuring the Carbon Intensity of AI in Cloud ...   \n",
       "3  2206.05802  2206.05802  Self-critiquing models for assisting human eva...   \n",
       "4  2206.06336  2206.06336     Language Models are General-Purpose Interfaces   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Few-shot learning is a challenging task that r...   \n",
       "1  Language models demonstrate both quantitative ...   \n",
       "2  By providing unprecedented access to computati...   \n",
       "3  We fine-tune large language models to write na...   \n",
       "4  Foundation models have received much attention...   \n",
       "\n",
       "                            source  \\\n",
       "0  http://arxiv.org/pdf/2206.02336   \n",
       "1  http://arxiv.org/pdf/2206.04615   \n",
       "2  http://arxiv.org/pdf/2206.05229   \n",
       "3  http://arxiv.org/pdf/2206.05802   \n",
       "4  http://arxiv.org/pdf/2206.06336   \n",
       "\n",
       "                                             authors  \\\n",
       "0  ['Yifei Li' 'Zeqi Lin' 'Shizhuo Zhang' 'Qiang ...   \n",
       "1  ['Aarohi Srivastava' 'Abhinav Rastogi' 'Abhish...   \n",
       "2  ['Jesse Dodge' 'Taylor Prewitt' 'Remi Tachet D...   \n",
       "3  ['William Saunders' 'Catherine Yeh' 'Jeff Wu' ...   \n",
       "4  ['Yaru Hao' 'Haoyu Song' 'Li Dong' 'Shaohan Hu...   \n",
       "\n",
       "                                    categories  \\\n",
       "0                            ['cs.CL' 'cs.AI']   \n",
       "1  ['cs.CL' 'cs.AI' 'cs.CY' 'cs.LG' 'stat.ML']   \n",
       "2                                    ['cs.LG']   \n",
       "3                            ['cs.CL' 'cs.LG']   \n",
       "4                                    ['cs.CL']   \n",
       "\n",
       "                                             comment  \\\n",
       "0                                               None   \n",
       "1  27 pages, 17 figures + references and appendic...   \n",
       "2  In ACM Conference on Fairness, Accountability,...   \n",
       "3                                               None   \n",
       "4  32 pages. The first three authors contribute e...   \n",
       "\n",
       "                                         journal_ref primary_category  \\\n",
       "0                                               None            cs.CL   \n",
       "1  Transactions on Machine Learning Research, May...            cs.CL   \n",
       "2                                               None            cs.LG   \n",
       "3                                               None            cs.CL   \n",
       "4                                               None            cs.CL   \n",
       "\n",
       "   published   updated                                         references  \\\n",
       "0   20220606  20230524  \\n\\n* D. Andor, L. He, K. Lee, and E. Pitler (...   \n",
       "1   20220609  20230612  \\n\\n* Wikiquote et al. (2021) Wikiquote, russi...   \n",
       "2   20220610  20220610  \\n\\n* (1)\\n* Anthony et al. (2020) Lasse F. Wo...   \n",
       "3   20220612  20220614   (RLHP) has become more common [1, 2, 3, 4], d...   \n",
       "4   20220613  20220613  \\n\\n* Agrawal et al. (2019) Harsh Agrawal, Kar...   \n",
       "\n",
       "                                             content  \\\n",
       "0  # Making Large Language Models Better Reasoner...   \n",
       "1  # Beyond the Imitation Game: Quantifying and e...   \n",
       "2  [MISSING_PAGE_EMPTY:1]\\n\\nIntroduction\\n\\nClim...   \n",
       "3  # Self-critiquing models for assisting human e...   \n",
       "4  # Language Models are General-Purpose Interfac...   \n",
       "\n",
       "                                       noref_content  \n",
       "0  # Making Large Language Models Better Reasoner...  \n",
       "1  # Beyond the Imitation Game: Quantifying and e...  \n",
       "2  [MISSING_PAGE_EMPTY:1]\\n\\nIntroduction\\n\\nClim...  \n",
       "3  # Self-critiquing models for assisting human e...  \n",
       "4  # Language Models are General-Purpose Interfac...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Hugging Face dataset\n",
    "dataset = load_dataset(\"deep-learning-analytics/arxiv_small_nougat\")\n",
    "\n",
    "# Convert to a Pandas DataFrame\n",
    "df = Dataset.to_pandas(dataset['train'])\n",
    "\n",
    "# Preview the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select subset of data to load into a database\n",
    "\n",
    "* Our key text column will be the `noref_content` which has the content of the paper without the references\n",
    "* We will include some metadata fields as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "keep_cols = ['id', 'title', 'authors', 'summary', 'source', 'published', 'noref_content']\n",
    "df_subset = df[keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset= df_subset.dropna()\n",
    "df_subset['noref_content'] = df_subset['noref_content'].str.replace('[^\\w\\s]', '', regex=True)\n",
    "\n",
    "df_subset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Documents using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DataFrameLoader\n",
    "loader = DataFrameLoader(df_subset, page_content_column=\"noref_content\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "text_splitter  = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separators=[\",\", \"\\n\"])\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Split Docs:  7596\n"
     ]
    }
   ],
   "source": [
    "print(\"Num Split Docs: \", len(split_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='To obtain the steplevel labels ie textlabel_ij for negative training data with wrong answers we design an algorithm that compares intermediate results among steps in positivenegative reasoning paths Figure 3 illustrates this algorithm This algorithm can not only work on math word problems but also generalize to other reasoning tasks we use an offtheshelf natural language inference model _robertalargemnli_Liu et al 2019 to check whether two reasoning steps are semantically equivalent or not Given a reasoning step if we cannot find any semantically equivalent step in the positive reasoning paths we label it and all the subsequent steps as negative steps\\n\\n 3 Experimental Setup\\n\\n Reasoning Tasks\\n\\nArithmetic ReasoningFollowing Wang et al 2022c we use AsDiv Miao et al 2020 SingleEq KoncelKedziorski et al 2015 MultiArith Roy and Roth 2015 SVAMP Patel et al 2021 and GSM8K Cobbe et al 2021', metadata={'id': 2206.02336, 'title': 'Making Large Language Models Better Reasoners with Step-Aware Verifier', 'authors': \"['Yifei Li' 'Zeqi Lin' 'Shizhuo Zhang' 'Qiang Fu' 'Bei Chen'\\n 'Jian-Guang Lou' 'Weizhu Chen']\", 'summary': 'Few-shot learning is a challenging task that requires language models to\\ngeneralize from limited examples. Large language models like GPT-3 and PaLM\\nhave made impressive progress in this area, but they still face difficulties in\\nreasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve\\ntheir reasoning skills, previous work has proposed to guide the language model\\nwith prompts that elicit a series of reasoning steps before giving the final\\nanswer, achieving a significant improvement on GSM8K from 17.9% to 58.1% in\\nproblem-solving rate. In this paper, we present DIVERSE (Diverse Verifier on\\nReasoning Step), a novel approach that further enhances the reasoning\\ncapability of language models. DIVERSE has three main components: first, it\\ngenerates diverse prompts to explore different reasoning paths for the same\\nquestion; second, it uses a verifier to filter out incorrect answers based on a\\nweighted voting scheme; and third, it verifies each reasoning step individually\\ninstead of the whole chain. We evaluate DIVERSE on the latest language model\\ncode-davinci-002 and show that it achieves new state-of-the-art results on six\\nof eight reasoning benchmarks (e.g., GSM8K 74.4% to 83.2%).', 'source': 'http://arxiv.org/pdf/2206.02336', 'published': 20220606})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the ChromaDB vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to load BGE Embeddings:  0.32668113708496094\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "### To create a new index\n",
    "# db_bge = Chroma.from_documents(split_docs, bge_embeddings, persist_directory=\"./chroma_db_bge\")\n",
    "### To load an already existing index\n",
    "db_bge = Chroma(persist_directory=\"./chroma_db_bge\", embedding_function=bge_embeddings)\n",
    "print(\"Time taken to load BGE Embeddings: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup OpenAI Embedding based Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to load OpenAI Embeddings:  0.006594181060791016\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# db_openai = Chroma.from_documents(split_docs, openai_embeddings, persist_directory=\"./chroma_db_openai\")\n",
    "### To load an already existing index\n",
    "db_openai = Chroma(persist_directory=\"./chroma_db_openai\", embedding_function=openai_embeddings)\n",
    "print(\"Time taken to load OpenAI Embeddings: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load time comparison:\n",
    "\n",
    "* Time taken to load BGE Embeddings:  352.92843294143677\n",
    "* Time taken to load OpenAI Embeddings:  48.999374866485596\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7596 in the OpenAI collection\n",
      "There are 7596 in the BGE collection\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", db_openai._collection.count(), \"in the OpenAI collection\")\n",
    "print(\"There are\", db_bge._collection.count(), \"in the BGE collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test both the DBs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BGE top 3 retrieved results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched do from BGE Embeddings at 1 is :  All our RL runs used the same hyperparameters as our prior work Bai et al 2022 However there are some differences The RLHF models for our earlier paper are finetuned from contextdistilled models while our current RLHF models are finetuned directly from pretrained models We didnt see much benefit to using context distillation since the improvement from RL was much more significant Furthermore the pretrained LMs that we use for all our runs have been improved since the prior work\n",
      "\n",
      "For PM comparison data we used 135296 HF helpfulness comparisons and 182831 constitutionallygenerated harmlessness comparisons one comparison generated for each SLCAI prompt For the purpose of doing controlled tests all the RL runs in this paper use the same set of training prompts which consists of all the HF and modelgenerated prompts used for SLCAI Section 32 plus _additional_ modelgenerated prompts 491142 for red team and 474300 for helpfulness\n",
      "\n",
      " Main Results /n ===========\n",
      "Matched do from BGE Embeddings at 2 is :  \n",
      "A successful example of RLHF used to teach a LM to use an external tool stems from _WebGPT_Nakano et al 2021 discussed in 323 a model capable of answering questions using a search engine and providing references to support such answers The tool interface is a simplified textbased webbrowser The model architecture is based on _GPT3_Brown et al 2020 and is trained to perform browsing actions expressed in natural language The model is finetuned on questionhuman demonstration pairs before further optimization via RLHF On two QA datasets _WebGPT_s answers are preferred relative to humangenerated ones and tend to be more factual than the original vanilla _GPT3_ model Similarly Menick et al 2022 propose _GopherCite_ a _Gopher_based LM model Rae et al 2021 finetuned with RLHF that can cite supporting evidence when answering questions and abstain from answering when unsure In contrast with _WebGPT_ _GopherCite_ uses an information retrieval external module rather than a webbrowser to find relevant information that improves its question answering capabilities Besides learning to use external tools RLHF has also proven useful for a wide range of language generation tasks from summarization Ziegler et al 2019 Wu et al 2021 Stiennon et al 2020 to training more helpful harmless and accurate assistants Glaese et al 2022 Cohen et al 2022 Ouyang et al 2022 Bai et al 2022 Since these works do not focus on training models to reason and act they are out of the scope of this survey /n ===========\n",
      "Matched do from BGE Embeddings at 3 is :  We use a series of language models pretrained in the way we described in prior work Bai et al 2022 As our goal is to train helpful and harmless assistants from _purely helpful_ assistants we use RLHF to train our initial helpful models For this we use the same process but using only helpfulness human feedback HF data However as a point of comparison we have also trained new preference models and helpful and harmless RLHF policies using human feedback /n ===========\n",
      "Matched do from BGE Embeddings at 4 is :  We conduct RLHF by first collecting human preference data for safety similar to Section 322 annotators write a prompt that they believe can elicit unsafe behavior and then compare multiple model responses to the prompts selecting the response that is safest according to a set of guidelines We then use the human preference data to train a safety reward model see Section 322 and also reuse the adversarial prompts to sample from the model during the RLHF stage /n ===========\n"
     ]
    }
   ],
   "source": [
    "query = \"What is RLHF? When can it be used?\"\n",
    "matched_docs = db_bge.similarity_search(query, k=8)\n",
    "\n",
    "# print results\n",
    "for index, value in enumerate(matched_docs):\n",
    "    pos = index+1\n",
    "    if index <=3:\n",
    "        print(f\"Matched do from BGE Embeddings at {pos} is : \", matched_docs[index].page_content, \"/n ===========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI top 3 retrieved results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched do from OpenAI Embeddings at 1 is :  RLHF works by using a pretrained LM to generate text which is then evaluated by humans by for example ranking two model generations for the same prompt This data is then collected to learn a reward model that predicts a scalar reward given any generated text The reward captures human preferences when judging model output Finally the LM is optimized against such reward model using RL policy gradient algorithms like PPO Schulman et al 2017 RLHF can be applied directly on top of a generalpurpose LM pretrained via selfsupervised learning However for more complex tasks the models generations may not be good enough In such cases RLHF is typically applied after an initial supervised finetuning phase using a small number of expert demonstrations for the corresponding downstream task Ramamurthy et al 2022 Ouyang et al 2022 Stiennon et al 2020 /n ===========\n",
      "Matched do from OpenAI Embeddings at 2 is :  We examine the influence of the amount of RLHF training for two reasons First RLHF 13 57 is an increasingly popular technique for reducing harmful behaviors in large language models 3 21 52 Some of these models are already deployed 52 so we believe the impact of RLHF deserves further scrutiny Second previous work shows that the amount of RLHF training can significantly change metrics on a wide range of personality political preference and harm evaluations for a given model size 41 As a result it is important to control for the amount of RLHF training in the analysis of our experiments\n",
      "\n",
      " Experiments\n",
      "\n",
      " 321 Overview /n ===========\n",
      "Matched do from OpenAI Embeddings at 3 is :  RLHF has emerged as a powerful strategy for finetuning Large Language Models enabling significant improvements in their performance Christiano et al 2017 The method first showcased by Stiennon et al 2020 in the context of textsummarization tasks has since been extended to a range of other applications In this paradigm models are finetuned based on feedback from human users thus iteratively aligning the models responses more closely with human expectations and preferences\n",
      "\n",
      "Ouyang et al 2022 demonstrates that a combination of instruction finetuning and RLHF can help fix issues with factuality toxicity and helpfulness that cannot be remedied by simply scaling up LLMs Bai et al 2022 partially automates this finetuningplusRLHF approach by replacing the humanlabeled finetuning data with the models own selfcritiques and revisions and by replacing human raters with a model when ranking model outputs in RLHF a process known as RL from AI Feedback RLAIF /n ===========\n",
      "Matched do from OpenAI Embeddings at 4 is :  We conduct RLHF by first collecting human preference data for safety similar to Section 322 annotators write a prompt that they believe can elicit unsafe behavior and then compare multiple model responses to the prompts selecting the response that is safest according to a set of guidelines We then use the human preference data to train a safety reward model see Section 322 and also reuse the adversarial prompts to sample from the model during the RLHF stage /n ===========\n"
     ]
    }
   ],
   "source": [
    "query = \"What is RLHF? When can it be used?\"\n",
    "matched_docs = db_openai.similarity_search(query, k=8)\n",
    "\n",
    "# print results\n",
    "for index, value in enumerate(matched_docs):\n",
    "    pos = index+1\n",
    "    if index <=3:\n",
    "        print(f\"Matched do from OpenAI Embeddings at {pos} is : \", matched_docs[index].page_content, \"/n ===========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation using Trulens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "from trulens_eval import Tru\n",
    "from trulens_eval.tru_custom_app import instrument\n",
    "tru = Tru()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a RAG class with trulens embedded in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG:\n",
    "    def __init__(self, db):\n",
    "        self.db = db\n",
    "        self.client = OpenAI()\n",
    "        self.messages = []\n",
    "        self.messages.append({\"role\": \"system\", \"content\":\"You are a friendly assistant who uses the provided context to answer the user's query\"})\n",
    "\n",
    "    @instrument\n",
    "    def retrieve(self, query: str) -> list:\n",
    "        matched_docs =self.db.similarity_search(query, k=6)\n",
    "        context_list = [doc.page_content for doc in matched_docs]\n",
    "        # context = '\\n'.join(context_list)\n",
    "        return context_list\n",
    "    \n",
    "    @instrument\n",
    "    def generate_completion(self, query: str, context: list) -> str:\n",
    "        self.messages.append({\"role\": \"user\", \"content\": f\"Use the provided context to answer user's query. \\n Context: {context} \\n Query: {query} \\n Answer:\"})\n",
    "        completion = self.client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=self.messages,\n",
    "        temperature=0.4,\n",
    "        )\n",
    "        result = completion.choices[0].message.content\n",
    "        return result\n",
    "    \n",
    "    @instrument\n",
    "    def query(self, query: str) -> str:\n",
    "        context_str = self.retrieve(query)\n",
    "        completion = self.generate_completion(query, context_str)\n",
    "        return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test responses using the 2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RLHF stands for Reinforcement Learning from Human Feedback. It is a strategy for finetuning Large Language Models (LLMs) based on feedback from human users, aligning the model's responses more closely with human expectations and preferences. RLHF can be used to train models to be both helpful and harmless without human feedback labels for harmlessness. It can also be used to improve model performance in various language generation tasks, such as text summarization, training more helpful, harmless, and accurate assistants, and teaching models to use external tools for tasks like question answering and providing references.\n"
     ]
    }
   ],
   "source": [
    "rag_bge = RAG(db_bge)\n",
    "query = \"What is RLHF? When can it be used?\"\n",
    "openai_response = rag_bge.query(query)\n",
    "print(openai_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RLHF stands for Reinforcement Learning from Human Feedback, and it is a technique used to align large language models (LLMs) with human preferences in order to make them more useful. RLHF works by using a pretrained language model to generate text, which is then evaluated by humans to learn a reward model that captures human preferences when judging model output. RLHF can be used directly on top of a general-purpose language model pretrained via self-supervised learning. However, for more complex tasks, RLHF is typically applied after an initial supervised fine-tuning phase using a small number of expert demonstrations for the corresponding downstream task. It has been shown to be effective in finetuning LLMs for tasks such as text summarization, addressing issues with factuality, toxicity, and helpfulness, and aligning model responses more closely with human expectations and preferences.\n"
     ]
    }
   ],
   "source": [
    "rag_openai = RAG(db_openai)\n",
    "query = \"What is RLHF? When can it be used?\"\n",
    "openai_response = rag_openai.query(query)\n",
    "print(openai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate this response using Trulens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup feedback functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In Groundedness, input source will be set to __record__.app.retrieve.rets.collect() .\n",
      "✅ In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Answer Relevance, input prompt will be set to __record__.app.retrieve.args.query .\n",
      "✅ In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Context Relevance, input question will be set to __record__.app.retrieve.args.query .\n",
      "✅ In Context Relevance, input statement will be set to __record__.app.retrieve.rets.collect() .\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval import Feedback, Select\n",
    "from trulens_eval.feedback import Groundedness\n",
    "from trulens_eval.feedback.provider.openai import OpenAI as fOpenAI\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Initialize provider class\n",
    "fopenai = fOpenAI()\n",
    "\n",
    "grounded = Groundedness(groundedness_provider=fopenai)\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons, name = \"Groundedness\")\n",
    "    .on(Select.RecordCalls.retrieve.rets.collect())\n",
    "    .on_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = (\n",
    "    Feedback(fopenai.relevance_with_cot_reasons, name = \"Answer Relevance\")\n",
    "    .on(Select.RecordCalls.retrieve.args.query)\n",
    "    .on_output()\n",
    ")\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(fopenai.qs_relevance_with_cot_reasons, name = \"Context Relevance\")\n",
    "    .on(Select.RecordCalls.retrieve.args.query)\n",
    "    .on(Select.RecordCalls.retrieve.rets.collect())\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the app\n",
    "Wrap the custom RAG with TruCustomApp, add list of feedbacks for eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Function <function RAG.generate_completion at 0x2c423e480> was not found during instrumentation walk. Make sure it is accessible by traversing app <__main__.RAG object at 0x38fd91110> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n",
      "Function <function RAG.query at 0x2c423e520> was not found during instrumentation walk. Make sure it is accessible by traversing app <__main__.RAG object at 0x38fd91110> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n",
      "Function <function RAG.retrieve at 0x2c423e3e0> was not found during instrumentation walk. Make sure it is accessible by traversing app <__main__.RAG object at 0x38fd91110> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n",
      "Function <function RAG.generate_completion at 0x2c423e480> was not found during instrumentation walk. Make sure it is accessible by traversing app <__main__.RAG object at 0x38fddaf10> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n",
      "Function <function RAG.query at 0x2c423e520> was not found during instrumentation walk. Make sure it is accessible by traversing app <__main__.RAG object at 0x38fddaf10> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n",
      "Function <function RAG.retrieve at 0x2c423e3e0> was not found during instrumentation walk. Make sure it is accessible by traversing app <__main__.RAG object at 0x38fddaf10> or provide a bound method for it as TruCustomApp constructor argument `methods_to_instrument`.\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval import TruCustomApp\n",
    "tru_rag_bge = TruCustomApp(rag_bge,\n",
    "    app_id = 'RAG BGE',\n",
    "    feedbacks = [f_groundedness, f_qa_relevance, f_context_relevance])\n",
    "\n",
    "tru_rag_openai = TruCustomApp(rag_openai,\n",
    "    app_id = 'RAG OpenAI',\n",
    "    feedbacks = [f_groundedness, f_qa_relevance, f_context_relevance])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a set of queries and run them through both the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"What is RLHF? When can it be used?\", \"How can the climate change impact of LLMs be estimated?\", \"Tell me more about the scaling laws of LLM training\", \"Explain diffusion models. What variants of them are there?\",\n",
    "           \"What kind of model is Stable Diffusion?\", \"Are their LLMs with Retrievel Augmented generation built in them?\", \"Show me different methods of prompting the LLMs and compare their benefits\", \"What is a Collaborative Language Model\",\n",
    "           \"Tell me about Audio based LLMs. Can some LLMs understand audio directly?\", \"Explain how multi modal LLMs work\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n",
      "Warning: model not found. Using cl100k_base encoding.\n",
      "Warning: model not found. Using cl100k_base encoding.\n",
      "Warning: model not found. Using cl100k_base encoding.\n",
      "Warning: model not found. Using cl100k_base encoding.\n",
      "Warning: model not found. Using cl100k_base encoding.\n",
      "Warning: model not found. Using cl100k_base encoding.\n",
      "Warning: model not found. Using cl100k_base encoding.\n",
      "Warning: model not found. Using cl100k_base encoding.\n",
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    }
   ],
   "source": [
    "for sample_query in queries:\n",
    "    with tru_rag_bge as recording:\n",
    "        rag_bge.query(sample_query)\n",
    "    with tru_rag_openai as recording:\n",
    "        rag_openai.query(sample_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n",
      "Config file already exists. Skipping writing process.\n",
      "Credentials file already exists. Skipping writing process.\n",
      "Dashboard already running at path:   Network URL: http://192.168.1.147:8501\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trulens_eval import Tru\n",
    "tru = Tru()\n",
    "tru.get_leaderboard(app_ids=[\"RAG BGE\", \"RAG OpenAI\"])\n",
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
