{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Calling Notebook\n",
    "\n",
    "This notebook walks through the steps to run function calling through OpenAI. Function Calling mimics what Langchain does internally with tools and agents. I think function calling is how chatgpt plugins work through OpenAI.\n",
    "\n",
    "To get started, please get your API keys for OpenAI and SERPAPI and add those to the environment fine *.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "import requests\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "from termcolor import colored\n",
    "from langchain import SerpAPIWrapper, OpenAI, LLMMathChain\n",
    "\n",
    "\n",
    "GPT_MODEL = \"gpt-3.5-turbo-0613\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for chat completion including functions\n",
    "The GPT 3.5 Turbo model under 0613 release has support for function calling.\n",
    "We are going to use OpenAI function calling directly instead of using Langchain\n",
    "We have defined 2 functions below, one for chat completion and the other to print roles with different colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def chat_completion_request(messages, functions=None, function_call=None, model=GPT_MODEL):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer \" + openai.api_key,\n",
    "    }\n",
    "    json_data = {\"model\": model, \"messages\": messages}\n",
    "    if functions is not None:\n",
    "        json_data.update({\"functions\": functions})\n",
    "    if function_call is not None:\n",
    "        json_data.update({\"function_call\": function_call})\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"https://api.openai.com/v1/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=json_data,\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(\"Unable to generate ChatCompletion response\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_conversation(messages):\n",
    "    role_to_color = {\n",
    "        \"system\": \"red\",\n",
    "        \"user\": \"green\",\n",
    "        \"assistant\": \"blue\",\n",
    "        \"function\": \"magenta\",\n",
    "    }\n",
    "    formatted_messages = []\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"system\":\n",
    "            formatted_messages.append(f\"system: {message['content']}\\n\")\n",
    "        elif message[\"role\"] == \"user\":\n",
    "            formatted_messages.append(f\"user: {message['content']}\\n\")\n",
    "        elif message[\"role\"] == \"assistant\" and message.get(\"function_call\"):\n",
    "            formatted_messages.append(f\"assistant: {message['function_call']}\\n\")\n",
    "        elif message[\"role\"] == \"assistant\" and not message.get(\"function_call\"):\n",
    "            formatted_messages.append(f\"assistant: {message['content']}\\n\")\n",
    "        elif message[\"role\"] == \"function\":\n",
    "            formatted_messages.append(f\"function ({message['name']}): {message['content']}\\n\")\n",
    "    for formatted_message in formatted_messages:\n",
    "        print(\n",
    "            colored(\n",
    "                formatted_message,\n",
    "                role_to_color[messages[formatted_messages.index(formatted_message)][\"role\"]],\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Function Calls\n",
    "\n",
    "We are going to have 2 functions\n",
    "* Search - Google Search for the user query\n",
    "* Calculator - Function that would do the math calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "        \"name\": \"search\",\n",
    "        \"description\": \"A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The search query\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"calculator\",\n",
    "        \"description\": \"Useful for when you need to answer questions about math.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The math query\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"]\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Call to OpenAI\n",
    "\n",
    "We will test the code by passing in the function arguments and making a first call to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': None,\n",
       " 'function_call': {'name': 'search',\n",
       "  'arguments': '{\\n  \"query\": \"Harry Styles age\"\\n}'}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_request = \"\"\"\n",
    "Find Harry Styles' age. What is their current age, power 2.3?\n",
    "\"\"\"\n",
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"You are a friendly chat assistant.\"})\n",
    "messages.append({\"role\": \"user\", \"content\": user_request})\n",
    "chat_response = chat_completion_request(\n",
    "    messages, functions=functions\n",
    ")\n",
    "assistant_message = chat_response.json()[\"choices\"][0][\"message\"]\n",
    "messages.append(assistant_message)\n",
    "assistant_message"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual function definitions\n",
    "\n",
    "OpenAI responds with the name of the function and the query to run through it. We need to define the actual functions that will run. \n",
    "I am going to leverage Langchain to run these functions easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_function_information(assistant_message):\n",
    "    \"\"\"Function to parse OpenAI assistant response and get information of which function to run.\"\"\"\n",
    "    fn_name = assistant_message['function_call']['name']\n",
    "    search_query = assistant_message['function_call']['arguments']\n",
    "    search_query = search_query.replace(\"\\n\",\"\")\n",
    "    search_query_dict = json.loads(search_query)\n",
    "    return fn_name, search_query_dict['query']\n",
    "\n",
    "def google_search(query):\n",
    "    \"\"\"Function to execute Google Search.\"\"\"\n",
    "    try:\n",
    "        search = SerpAPIWrapper()\n",
    "        results = search.run(query)\n",
    "    except Exception as e:\n",
    "        results = f\"query failed with error: {e}\"\n",
    "    return results\n",
    "\n",
    "def math_calculator(query):\n",
    "    \"\"\"Function to run Math Calculations.\"\"\"\n",
    "    try:\n",
    "        llm = OpenAI(temperature=0)\n",
    "        calculator = LLMMathChain.from_llm(llm=llm)\n",
    "        results = calculator.run(query)\n",
    "    except Exception as e:\n",
    "        results = f\"query failed with error: {e}\"\n",
    "    return results\n",
    "\n",
    "\n",
    "def function_executor(assistant_message):\n",
    "    \"\"\"Tie above functions together so either can be executed\"\"\"\n",
    "    name, query = extract_function_information(assistant_message)\n",
    "    if name == 'search':\n",
    "        results = google_search(query)\n",
    "    elif name == 'calculator':\n",
    "        results = math_calculator(query)\n",
    "    else:\n",
    "        results = f\"Error: function {name} does not exist\"\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run this all together with a while loop that keeps sending messages to OpenAI till the user has their answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31msystem: You are a friendly chat assistant.\n",
      "\u001b[0m\n",
      "\u001b[32muser: \n",
      "Find Harry Styles' age. What is their current age power 2.3?\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34massistant: {'name': 'search', 'arguments': '{\\n  \"query\": \"Harry Styles age\"\\n}'}\n",
      "\u001b[0m\n",
      "\u001b[35mfunction (search): 29 years\n",
      "\u001b[0m\n",
      "\u001b[34massistant: {'name': 'calculator', 'arguments': '{\\n  \"query\": \"29 power 2.3\"\\n}'}\n",
      "\u001b[0m\n",
      "\u001b[35mfunction (calculator): Answer: 2309.486325717843\n",
      "\u001b[0m\n",
      "\u001b[34massistant: Harry Styles is currently 29 years old. Their current age to the power of 2.3 is approximately 2309.49.\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "user_request = \"\"\"\n",
    "Find Harry Styles' age. What is their current age power 2.3?\n",
    "\"\"\"\n",
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"You are a friendly chat assistant.\"})\n",
    "messages.append({\"role\": \"user\", \"content\": user_request})\n",
    "keep_running = True\n",
    "while keep_running:\n",
    "    chat_response = chat_completion_request(\n",
    "        messages, functions=functions\n",
    "    )\n",
    "    assistant_message = chat_response.json()[\"choices\"][0][\"message\"]\n",
    "    messages.append(assistant_message)\n",
    "    if assistant_message.get(\"function_call\"):\n",
    "        results = function_executor(assistant_message)\n",
    "        messages.append({\"role\": \"function\", \"name\": assistant_message[\"function_call\"][\"name\"], \"content\": results})\n",
    "        keep_running = True\n",
    "    else:\n",
    "        keep_running = False\n",
    "pretty_print_conversation(messages)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try again with a more complex request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openai_function_example(user_query):\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"system\", \"content\": \"You are a friendly chat assistant.\"})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_request})\n",
    "    keep_running = True\n",
    "    while keep_running:\n",
    "        chat_response = chat_completion_request(\n",
    "            messages, functions=functions\n",
    "        )\n",
    "        assistant_message = chat_response.json()[\"choices\"][0][\"message\"]\n",
    "        messages.append(assistant_message)\n",
    "        if assistant_message.get(\"function_call\"):\n",
    "            results = function_executor(assistant_message)\n",
    "            messages.append({\"role\": \"function\", \"name\": assistant_message[\"function_call\"][\"name\"], \"content\": results})\n",
    "            keep_running = True\n",
    "        else:\n",
    "            keep_running = False\n",
    "    pretty_print_conversation(messages)\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31msystem: You are a friendly chat assistant.\n",
      "\u001b[0m\n",
      "\u001b[32muser: \n",
      "How old is Donald trump in 2023 ? How old is Biden in 2023 and who is younger by how much?\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34massistant: To determine the age of Donald Trump and Joe Biden in 2023, we need to know their birth dates. However, as of now, it is 2021 and their birth dates are well-known. \n",
      "\n",
      "Donald Trump was born on June 14, 1946, and Joe Biden was born on November 20, 1942. \n",
      "\n",
      "In 2023: \n",
      "\n",
      "- Donald Trump will be 77 years old (assuming his birthday has already passed). \n",
      "- Joe Biden will be 81 years old (assuming his birthday has already passed). \n",
      "\n",
      "Therefore, in 2023, Joe Biden will be older than Donald Trump by 4 years.\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "user_request = \"\"\"\n",
    "How old is Donald trump in 2023 ? How old is Biden in 2023 and who is younger by how much?\n",
    "\"\"\"\n",
    "messages = openai_function_example(user_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31msystem: You are a friendly chat assistant.\n",
      "\u001b[0m\n",
      "\u001b[32muser: \n",
      "What is current US unemployment rate? If it decreases by 25%, what will it be then?\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34massistant: {'name': 'search', 'arguments': '{\\n  \"query\": \"current US unemployment rate\"\\n}'}\n",
      "\u001b[0m\n",
      "\u001b[35mfunction (search): The unemployment rate increased by 0.3 percentage point to 3.7 percent in May, and the number of unemployed persons rose by 440,000 to 6.1 ...\n",
      "\u001b[0m\n",
      "\u001b[34massistant: The current US unemployment rate is 3.7%. If it decreases by 25%, the new unemployment rate will be approximately 2.775%.\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "user_request = \"\"\"\n",
    "What is current US unemployment rate? If it decreases by 25%, what will it be then?\n",
    "\"\"\"\n",
    "messages = openai_function_example(user_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
